{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Machine Learning: Week 8\n",
    "## Decision trees and Random forest\n",
    "\n",
    "This week we will learn something about trees and forests for regression and classification. We will start with regression trees. Please download all files from blackboard before starting the notebook (the notebook itself + `regression_train.csv`, `regression_test.csv`). Also, execute each code cell in the correct order. \n",
    "\n",
    "Please read over the whole notebook. It **contains several solved exercises and three (3) that you need to add some code to**. If something does not run it might mean you need to alter or add a statement :) Also, **make sure you run all the code cells** (even the ones with examples) to ensure the rest of the notebook runs as intended.\n",
    "\n",
    "We'll cover the following topics:\n",
    "- [Regression Trees](#Regression-Trees)\n",
    "    - [Training a Regression Tree](#Training-a-Regression-Tree)\n",
    "        - [Exercise 1a: Training the Tree](#Exercise-1a:-Training-the-Tree)\n",
    "    - [Evaluating the Tree](#Evaluating-the-Tree)\n",
    "        - [Exercise 1b: Finding the best fit](#Exercise-1b:-Finding-the-best-fit)\n",
    "- [Decision Trees](#Decision-Trees)\n",
    "    - [Training a Decision Tree](#Training-a-Decision-Tree)\n",
    "        - [Exercise 2a: Decision boundary analysis](#Exercise-2a:-Decision-boundary-analysis)\n",
    "    - [Evaluating Decision Trees](#Evaluating-Decision-Trees)\n",
    "        - [Exercise 2b: Qualitative analysis](#Exercise-2b:-Qualitative-analysis)\n",
    "- [Decision Forests](#Decision-Forests)\n",
    "    - [Exercise 3a: Qualitative analysis of `n_estimators`](#Exercise-3a:-Qualitative-analysis-of-n_estimators)\n",
    "    - [Exercise 3b: Quantitative analysis of `min_samples_leaf`](#Exercise-3b:-Quantitative-analysis-of-min_samples_leaf)\n",
    "    - [Exercise 3c: Quantitative analysis of `n_estimators`](#Exercise-3c:-Quantitative-analysis-of-n_estimators)\n",
    "- [Feedback?](https://forms.office.com/e/YYd8G5d5cB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Regression Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "We start with a simple regression task. We have to learn a continous function with a 1-dimensional input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "We first load the training data with the pandas data frames and plot the training points and the \n",
    "ground-truth function. The ground-truth values are stored in the test data set in order to evaluate the quality \n",
    "of our fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot\n",
    "import numpy as np\n",
    "\n",
    "data_train = pd.read_csv('regression_train.csv')\n",
    "data_test = pd.read_csv('regression_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = data_train['x'].values\n",
    "y_train = data_train['y'].values\n",
    "\n",
    "x_test = data_test['x'].values\n",
    "y_test = data_test['y'].values\n",
    "\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "x_test = x_test.reshape(-1, 1)\n",
    "\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "plt.plot(x_train,y_train, 'bo')\n",
    "plt.plot(x_test,y_test, 'g')\n",
    "plt.legend(('training points', 'ground truth'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Training a Regression Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "We will use the `sklearn` package to train our regression trees. `sklearn` is a generic machine learning library \n",
    "that offers a lot of learning algorithms. A regression tree can be generated by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "regTree = tree.DecisionTreeRegressor(min_samples_leaf=1, max_depth=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "We can set the minimum number of samples per leaf in the tree and the maximum depth of the tree as can be seen above. \n",
    "\n",
    "The tree can be trained by using the `.fit` method, similarly to most other models from `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regTree = regTree.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "We can use the trained tree for prediction calling the `predict` method (again, similar for most `sklearn` models):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = regTree.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "#### Exercise 1a: Training the Tree\n",
    "\n",
    "In this excercise you are supposed to train the tree for our regression task.\n",
    "- Train a tree with `min_samples_leaf` set to 1, 5 and 10 and predict the output for `x_test` and plot the predicted\n",
    "function values (store these is `y_predict1`, `y_predict2` and `y_predict3`\n",
    "- Do you see a difference in the functions?\n",
    "- Based on this qualitative analysis (visual inspection), which value of `min_samples_leaf` would you use? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "######################################\n",
    "##### YOUR CODE GOES HERE: ###########\n",
    "######################################\n",
    "# your code here: tree with min_samples_leaf = 1:\n",
    "# initialise model:\n",
    "# regTree1 = \n",
    "# fit the model:\n",
    "# \n",
    "# predict the values:\n",
    "# y_predict1 = \n",
    "#\n",
    "# tree with min_samples_leaf = 5:\n",
    "# initialise model:\n",
    "#\n",
    "# predict the values:\n",
    "#\n",
    "# tree with min_samples_leaf = 10:\n",
    "# initialise model:\n",
    "#\n",
    "# predict the values:\n",
    "#\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(3,1,figsize=(7, 18))\n",
    "\n",
    "# plot for min_samples = 1:\n",
    "axs[0].plot(x_train,y_train, 'bo')\n",
    "axs[0].plot(x_test,y_test, 'g')\n",
    "# plot the predicted values:\n",
    "axs[0].plot(x_test, y_predict1, 'r')\n",
    "axs[0].set_title('min_samples = 1')\n",
    "\n",
    "# plot for min_samples = 5:\n",
    "axs[1].plot(x_train,y_train, 'bo')\n",
    "axs[1].plot(x_test,y_test, 'g')\n",
    "# plot the predicted values:\n",
    "axs[1].plot(x_test, y_predict2, 'r')\n",
    "axs[1].set_title('min_samples = 5')\n",
    "\n",
    "\n",
    "# plot for min_samples = 10:\n",
    "axs[2].plot(x_train,y_train, 'bo')\n",
    "axs[2].plot(x_test,y_test, 'g')\n",
    "# plot the predicted values:\n",
    "axs[2].plot(x_test, y_predict3, 'r')\n",
    "axs[2].set_title('min_samples = 10')\n",
    "\n",
    "#plt.savefig('regressiontrees.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Evaluating the Trees\n",
    "\n",
    "We can also use `sklearn` to evaluate the tree. There are different metrics that we can use. We will use the \n",
    "mean absolute error criterion to evaluate the trees. We can compute the MAE on the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "y_predict_train1 = regTree1.predict(x_train)\n",
    "mseTrainTree1 = metrics.mean_absolute_error(y_train, y_predict_train1)\n",
    "mseTrainTree2 = metrics.mean_absolute_error(y_train, regTree2.predict(x_train))\n",
    "mseTrainTree3 = metrics.mean_absolute_error(y_train, regTree3.predict(x_train))\n",
    "\n",
    "mseTrainTree1, mseTrainTree2, mseTrainTree3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "And we can compute the MSE on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "mseTestTree1 = metrics.mean_absolute_error(y_test, y_predict1)\n",
    "mseTestTree2 = metrics.mean_absolute_error(y_test, regTree2.predict(x_test))\n",
    "mseTestTree3 = metrics.mean_absolute_error(y_test, regTree3.predict(x_test))\n",
    "\n",
    "mseTestTree1, mseTestTree2, mseTestTree3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "#### Exercise 1b: Finding the best fit\n",
    "We can see that trees with `min_samples_leaf = 1` have MSE 0 on the training set as the training set is learned by heart. However, the error on the test set is the one that really counts. Here, also `min_samples_leaf = 1` performs the best, but **can you find better settings of `min_samples_leaf`**?\n",
    "- Go back to [Exercise 1a](#Exercise-1a:-Training-the-Tree) and try different values of `min_samples_leaf`. Then use the code in [Evaluating the trees](#Evaluating-the-Trees) to evaluate these on the training and the test set.\n",
    "- Can you find a value `min_samples_leaf` which results in a smaller **MSE on the test set** than `min_samples_leaf=1`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Decision Trees\n",
    "In contrast to regression trees, the output for a decision tree is a discrete class label, not \n",
    "a continuous value. A decision tree can be used for any multi-label classification problem. We will use the \n",
    "decision trees on the iris data set. We do **not** use the same features as in lecture examples.\n",
    "Instead, we will only use the first two features `sepal_length` and `sepal_width` in the training data as input\n",
    "for illustration purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data[:,:2]\n",
    "Y = iris.target\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "\n",
    "# Plot the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Set1)\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.ylabel(iris.feature_names[1])\n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "The iris data set contains 3 classes of iris flowers that should be classified according to their sepal width \n",
    "and sepal length.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "We split the data set into 67% training data and 33% test data. I.e., we have 100 training and 50 test points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Training a Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "We will again use the `sklearn` package. A decision tree can be generated by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "decTree = tree.DecisionTreeClassifier(min_samples_leaf=2, max_depth=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "We can set the same properties as for a regression tree. Similarly, we can train the tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decTree = decTree.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "We can use the trained tree for prediction by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = decTree.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "#### Exercise 2a: Decision boundary analysis\n",
    "In this excercise we want to train a decision tree for different values of `min_samples_leaf`. After training the tree,\n",
    "we will plot the decision boundary of the learned tree with the existing python code.\n",
    "- Plot the decision boundary for different number of `min_samples_leaf`.\n",
    "- Can you observe a qualitatitve difference between the learned classifiers?\n",
    "- Which value of `min_samples_leaf` would you choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "n_classes = 3\n",
    "plot_step = 0.02\n",
    "\n",
    "######################################\n",
    "##### CHANGE THE FOLLOWING LINE: #####\n",
    "######################################\n",
    "clf = DecisionTreeClassifier(min_samples_leaf=5)\n",
    "\n",
    "clf = clf.fit(X_train, Y_train)  \n",
    "\n",
    "# create a grid for the two input dimensions\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "plt.figure()\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Pastel1)\n",
    "\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=plt.cm.Set1)\n",
    "\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.ylabel(iris.feature_names[1])\n",
    "plt.axis(\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Evaluating Decision Trees\n",
    "For evaluating the decision tree, we can compute the ratio of correctly \n",
    "classified samples on the test set. This metric is called  `accuracy_score` in sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "train_accuracy = metrics.accuracy_score(Y_train, clf.predict(X_train))\n",
    "test_accuracy = metrics.accuracy_score(Y_test, clf.predict(X_test))\n",
    "\n",
    "train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "#### Exercise 2b: Qualitative analysis\n",
    "\n",
    "For the values of `min_samples_leaf = [1,2,3,5,7,10, 15, 20, 50]`, compute the `accuracy_score` on the train and on \n",
    "the test set. Plot both accuracy scores as a function of `min_samples_leaf`. You should be able to identify areas where the model is **overfitting** and **underfitting** on the plot:\n",
    "- _Overfitting_ is characterised by a high training accuracy but a low (and improving) testing accuracy. For which values of `min_samples_leaf` is the model _overfitting_?\n",
    "- _Underfitting_ happens when both training and testing performance are low/degrading. For which values of `min_samples_leaf` is the model _underfiting_?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "minSamples = [1,2,3,5,7,10, 15, 20, 50]\n",
    "train_accuracy = np.zeros((len(minSamples),1))\n",
    "test_accuracy = np.zeros((len(minSamples),1))\n",
    "\n",
    "\n",
    "for i in range(0,len(minSamples)):\n",
    "   \n",
    "    ######################################\n",
    "    ##### YOUR CODE GOES HERE: ###########\n",
    "    ######################################\n",
    "    # declare a decisionTreeClassifier taking as input the minSamples:\n",
    "      \n",
    "    # fit it to your data:\n",
    "\n",
    "    # store the train and test accuracy:\n",
    "    #train_accuracy[i] = ...\n",
    "    #test_accuracy[i] = ...\n",
    "    \n",
    "\n",
    "plt.figure()\n",
    "plt.plot(minSamples, train_accuracy, 'b')\n",
    "plt.plot(minSamples, test_accuracy, 'g')\n",
    "plt.xlabel('min_samples_per_leaf')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(('training set', 'test set'))\n",
    "#plt.savefig('classification_minSamples.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Decision Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "We will again use the sklearn package. A decision forest can be generated by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "decForest = ensemble.RandomForestClassifier(n_estimators=10, min_samples_leaf=2, max_depth=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "We can set the same properties as for a decision forest (including number of trees by `n_estimator`). \n",
    "Similarly, we can train the forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decForest = decForest.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "We can use the trained tree for prediction by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = decForest.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "#### Exercise 3a: Qualitative analysis of `n_estimators`\n",
    "In this excercise we want to train a decision forests for different values of `n_estimators`. After training the forest, we will plot the decision boundary of the learned forest with the existing python code. \n",
    "- Plot the decision boundary for different number of `n_estimators` and use `min_samples_leaf = 10`.\n",
    "- Can you observe a qualitatitve difference between the learned classifiers?\n",
    "- Execute your code several times. Can you observe a difference between the executions? If yes, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "n_classes = 3\n",
    "plot_colors = \"bry\"\n",
    "plot_step = 0.02\n",
    "\n",
    "\n",
    "######################################\n",
    "##### CHANGE THE FOLLOWING LINE: #####\n",
    "######################################\n",
    "clf = ensemble.RandomForestClassifier(n_estimators=30, min_samples_leaf=10, max_depth=None)\n",
    "\n",
    "######################################\n",
    "##### YOUR CODE GOES HERE: ###########\n",
    "######################################\n",
    "# fit the model to the data:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Pastel1)\n",
    "\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap=plt.cm.Set1)\n",
    "\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.ylabel(iris.feature_names[1])\n",
    "plt.axis(\"tight\")\n",
    "#plt.savefig('classification_forest.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3b: Quantitative analysis of `min_samples_leaf`\n",
    "\n",
    "The below cell should similarly examine the effect of varying `min_samples_leaf` for RandomForests:\n",
    "- Fill in the missing parts of the code, making sure you repeat the training/testing process for each configuration `numTrials` times (which is set to 10). This is similar to our [previous analysis on trees](#Exercise-2b:-Qualitative-analysis) (with an additional nested `for` loop repeating the training `numTrials` times).\n",
    "- The implementation is split in two cells as the first cell can take a while to run. Once you are satisfied with your implementation, **run the second cell** to visually examine the effect of varying `min_samples_leaf`.\n",
    "- Taking into account what you know about overfitting and underfitting, which value of `min_sample_leaf` would you chose based on the below graph?\n",
    "- If you run the below cells of code multiple times, does the graph change? Why? Does that change your choice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "minSamples = [1,3,5,7,10, 15, 20, 30]\n",
    "\n",
    "numTrials = 10\n",
    "train_accuracy_single = np.empty((len(minSamples),numTrials))\n",
    "test_accuracy_single = np.empty((len(minSamples),numTrials))\n",
    "\n",
    "# store the mean test/train accuracy for all the 10 trials for a certain value of min_samples\n",
    "train_accuracy_mean = np.empty((len(minSamples),1))\n",
    "test_accuracy_mean = np.empty((len(minSamples),1))\n",
    "\n",
    "# store the standard deviation of test/train accuracy for all the 10 trials for a certain value of min_samples\n",
    "train_accuracy_std = np.empty((len(minSamples),1))\n",
    "test_accuracy_std = np.empty((len(minSamples),1))\n",
    "\n",
    "\n",
    "for i in range(0,len(minSamples)):\n",
    "    ######################################\n",
    "    ##### YOUR CODE GOES HERE: ###########\n",
    "    ######################################\n",
    "    # declare a RandomForestClassifier taking as input minSamples[i], and using 100 estimators:\n",
    "    \n",
    "\n",
    "    # repeat each experiment numTrials times:\n",
    "\n",
    "        # fit the classifier to your data\n",
    "\n",
    "        # store the train and test accuracy for the model using minSamples[i], for the trial j:\n",
    "        # train_accuracy_single[i,j] = ...\n",
    "        # test_accuracy_single[i, j] = ...\n",
    "\n",
    "    # the below code calculates the mean and standard deviation across\n",
    "    # your numTrials trials and should not be changed\n",
    "    train_accuracy_mean[i] = np.mean(train_accuracy_single[i,:])\n",
    "    train_accuracy_std[i] = np.std(train_accuracy_single[i,:])\n",
    "    \n",
    "    test_accuracy_mean[i] = np.mean(test_accuracy_single[i,:])\n",
    "    test_accuracy_std[i] = np.std(test_accuracy_single[i,:])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: the errorbar function of matplotlib has undergone recent changes. If the below code does not work for you because you're running an older version of matplotlib, you can use the commented lines which do not plot the error bars instead, they should definitely work._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.errorbar(minSamples, train_accuracy_mean.squeeze(), yerr = train_accuracy_std.ravel(), label='train')\n",
    "plt.errorbar(minSamples, test_accuracy_mean.squeeze(), yerr = test_accuracy_std.ravel(), label='test')\n",
    "#plt.plot(minSamples, train_accuracy_mean, label='train')\n",
    "#plt.plot(minSamples, test_accuracy_mean, label='test')\n",
    "plt.legend()\n",
    "plt.xlabel('min_samples_per_leaf')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "#### Exercise 3c: Quantitative analysis of `n_estimators`\n",
    "\n",
    "Test the algorithm for a different number of trees, i.e., [1,5,10,20,40,60,100]. Repeat each experiment 10 times and average over the performance values due to the randomness. Evaluate the average accuracy on the training and on the test set for the given number of trees (`n_estimators`). \n",
    "Use `min_samples_leaf = 15`.\n",
    "- Fill in the missing parts of the code, making sure you repeat the training/testing process for each configuration `numTrials` times (which is set to 10). This is similar to the [previous exercise](#Exercise-3b:-Quantitative-analysis-of-min_samples_leaf).\n",
    "- Which value of `n_estimators` would you chose? Has your choice changed from [Exercise 3a](#Exercise-3a:-Qualitative-analysis-of-n_estimators)?\n",
    "- Can you comment on the behaviour of the classifier for `n_estimator < 50`?\n",
    "- Can you comment on the behaviour of the classifier for `n_estimator > 200`?\n",
    "\n",
    "_Note: the below cells might take a few moments to execute._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "numTrees  = [1,5,10,20,40,60,100]\n",
    "\n",
    "numTrials = 10\n",
    "# store test/train accuracy for every trial\n",
    "train_accuracy_single = np.empty((len(numTrees),numTrials))\n",
    "test_accuracy_single = np.empty((len(numTrees),numTrials))\n",
    "\n",
    "# store the mean test/train accuracy for all the 10 trials for a certain value of n_estimators\n",
    "train_accuracy_mean = np.empty((len(numTrees),1))\n",
    "test_accuracy_mean = np.empty((len(numTrees),1))\n",
    "\n",
    "# store the standard deviation of test/train accuracy for all the 10 trials for a certain value of n_estimators\n",
    "train_accuracy_std = np.empty((len(numTrees),1))\n",
    "test_accuracy_std = np.empty((len(numTrees),1))\n",
    "\n",
    "for i in range(0,len(numTrees)):\n",
    "    ######################################\n",
    "    ##### YOUR CODE GOES HERE: ###########\n",
    "    ######################################\n",
    "    # declare a RandomForestClassifier taking as input numTrees[i], and using minimum 15 samples per leaf\n",
    "    \n",
    "\n",
    "    # repeat each experiment numTrials times:\n",
    "\n",
    "        # fit the classifier to your data\n",
    "\n",
    "        # store the train and test accuracy for the model using numTrees[i], for the trial j:\n",
    "        # train_accuracy_single[i,j] = ...\n",
    "        # test_accuracy_single[i, j] = ...\n",
    "\n",
    "    # the below code calculates the mean and standard deviation across your numTrials trials\n",
    "    train_accuracy_mean[i] = np.mean(train_accuracy_single[i,:])\n",
    "    train_accuracy_std[i] = np.std(train_accuracy_single[i,:])\n",
    "    \n",
    "    test_accuracy_mean[i] = np.mean(test_accuracy_single[i,:])\n",
    "    test_accuracy_std[i] = np.std(test_accuracy_single[i,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: the errorbar function of matplotlib has undergone recent changes. If the below code does not work for you, you can use the commented lines which do not plot the error bars instead, they should definitely work._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "#plt.plot(numTrees, train_accuracy_mean, 'b', label='train')\n",
    "#plt.plot(numTrees, test_accuracy_mean, 'g', label='test')\n",
    "plt.errorbar(numTrees, train_accuracy_mean.squeeze(), yerr = train_accuracy_std.ravel(),  label='train')\n",
    "plt.errorbar(numTrees, test_accuracy_mean.squeeze(), yerr = test_accuracy_std.ravel(), label ='test')\n",
    "plt.legend()\n",
    "# Always label your axes\n",
    "plt.xlabel(\"numTrees\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "\n",
    "\n",
    "#plt.savefig('classification_forest_numtrees.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
