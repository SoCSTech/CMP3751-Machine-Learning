{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning: Week 9\n",
    "\n",
    "## Probabilistic models, Bayes' classifier, Naive Bayes' classifier\n",
    "\n",
    "This week we introduced probabilistic machine learning models in the lecture. In this workshop, we will look into **Naive Bayes' classifier** in more defail. Please download all files from blackboard before starting the notebook (the notebook itself + `categorical_jams.csv`, `fires_bejaia_ML.csv`).\n",
    "\n",
    "We will go over the notebook together in the workshop. It contains several solved examples and some **exercises where you need to add some code**. If something does not run it might mean you need to alter or add a statement :) Also, **make sure you run all the code cells** (even the ones with examples) to ensure the rest of the notebook runs as intended.\n",
    "\n",
    "We'll cover the following topics:\n",
    "- [Categorical Naive Bayes' classifier](#Categorical-Naive-Bayes'-classifier)\n",
    "    - [Step-by-step implementation](#Step-by-step-implementation)\n",
    "        - [Exercise 1a](#Exercise-1a)\n",
    "        - [Exercise 1b: Priors](#Exercise-1b:-Priors)\n",
    "        - [Exercise 1c: Likelihoods](#Exercise-1c:-Likelihoods)\n",
    "        - [Exercise 1d: Posteriors](#Exercise-1d:-Posteriors)\n",
    "    - [Zero probability problem](#Zero-probability-problem-(extension))\n",
    "        - [Exercise 2: Zero probability correction **(extension)**](#Exercise-2:-Zero-probability-correction-(extension))\n",
    "- [Gaussian Naive Bayes' classifier](#Gaussian-Naive-Bayes'-classifier)\n",
    "    - [Exercise 3a: Training GaussianNB](#Exercise-3a:-Training-GaussianNB)\n",
    "    - [Exercise 3b: Updating beliefs](#Exercise-3b:-Updating-beliefs)\n",
    "    - [Exercise 3c: Removing correlated features](#Exercise-3c:-Removing-correlated-features)\n",
    "- [Making a class **(extension)**](#Making-a-class-(extension))\n",
    "    - [Exercise 4: Implementing a class **(extension)**](#Exercise-4:-Implementing-a-class-(extension))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Naive Bayes' classifier\n",
    "\n",
    "Let us first look at a smiple toy dataset containing information about trafic jams on a certain road. \n",
    "\n",
    "In the dataset, three **types of featres** are recorded, with the following possible values:\n",
    "- `weather`: {`Clear`, `Rainy`, `Snowy`}\n",
    "- `timeOfWeek`: {`Workday`, `Weekend`}\n",
    "- `timeOfDay`: {`Morning`, `Lunch`}\n",
    "\n",
    "We will try to do this by manipulating [`pandas.DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) directly.\n",
    "\n",
    "First, let's input the data and examine the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jams_df = pd.read_csv('categorical_jams.csv')\n",
    "for column in jams_df.columns:\n",
    "    jams_df[column] = jams_df[column].astype('category')\n",
    "jams_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-step implementation\n",
    "\n",
    "First, try to implement the Naive Bayes' classifier step-by-step. The formulas and appropriate pandas methods will be indicated in the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1a\n",
    "\n",
    "Store the name of the column containing our labels (= ground truth) in a variable called `label`, and the names of all the features in the variable called `features`. Assume the ground truth label is the last column in the dataset.\n",
    "\n",
    "To retrieve the names of the columns in the [`pandas.DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), you can use the [`pandas.DataFrame.columns`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.columns.html) attribute (so, applied to our example, you will need to access elements of `jams_df.columns`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_col = ...\n",
    "# feature_cols = ...\n",
    "\n",
    "print(\"Label = {}, features = {}\".format(label_col, feature_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1b: Priors\n",
    "\n",
    "Calculate the **priors** for both of the classes (_i.e. both possible values of the `label_col` column_).\n",
    "\n",
    "Remember, the prior probability of a class is _estimated from the dataset_ as the **number of samples of that class** in the dataset **divided by the total number of samples**:\n",
    "$$\n",
    "P(class = \\texttt{No}) = \\frac{N(class=\\texttt{No})}{N},\n",
    "$$\n",
    "where $N$ is the total number of samples in the dataset, and $N(class==\\texttt{No})$ is the number of samples of class `No`.\n",
    "\n",
    "To implement this in with `pandas`, you can use [`pandas.DataFrame.value_counts()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.value_counts.html) method on the ground truth (`label_col`) column to obtain the total number of smaples from each class. To retrieve the total number of samples, you can look into [`pandas.DataFrame.shape`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shape.html) attribute, or use [`pandas.DataFrame.count()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.count.html) on the ground truth column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# priors = ...\n",
    "display(priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1c: Likelihoods\n",
    "\n",
    "Write a function `likelihood` which calculates the **likelihoods** for all possible values of a feature. As a reminder, the _likelihood_ of e.g. a certain weather condition _given_ the state of traffic (jam or not jam), can be _estimated from the data_ as:\n",
    "\n",
    "$$\n",
    "P(\\texttt{Rainy}|class=\\texttt{Yes}) = \\frac{N(\\texttt{Rainy}, class=\\texttt{Yes})}{N(class=\\texttt{Yes})},\n",
    "$$\n",
    "where $N(\\texttt{Rainy}, class=\\texttt{No})$ is the number of samples of class `Yes` where the weather was `Rainy`.\n",
    "\n",
    "This needs to be calculated for **every possible value** of every feature. Fortunately, `pandas` provides the [`pandas.Dataframe.groupby()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) method which allows performing such calculations for _all classes at once_ (passing the argument `observed=True`). You will need to combine this with  [`pandas.DataFrame.value_counts()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.value_counts.html) and [`pandas.DataFrame.count()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.count.html), similarly to the [previous exercise](#Exercise-1b:-Priors).\n",
    "\n",
    "Your function should return a `DataFrame` indexed by a [`pandas.MultiIndex`](https://pandas.pydata.org/docs/reference/api/pandas.MultiIndex.html#pandas.MultiIndex) -- i.e. the indexing column will contain both the ground truth `label`s, and the possible realisations of `feature`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(feature):\n",
    "    ##########################################################\n",
    "    ################## YOUR SOLUTION GOES HERE ###############\n",
    "    ##########################################################\n",
    "    # group your dataframe (using groupby()) according to the label column (in label_col)\n",
    "    # select only the feature of interest from the result\n",
    "    # finally, calculate value_counts on the result to get the number of occurances\n",
    "    # of each ground truth label for each value of the feature (e.g. 'Lunch' 'Evening' 'Morning')\n",
    "    # finally, either a) pass normalize=True to the value_counts function to get frequencies\n",
    "    # instead of counts, or b) divide the above value with the counts (obtained on the grouped\n",
    "    # data frame after selecting the appropriate feature)\n",
    "\n",
    "    # remove/replace this line\n",
    "    return 0\n",
    "\n",
    "print(likelihood('timeOfDay'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1d: Posteriors\n",
    "\n",
    "Write a (non-probabilistic) function `classify` which can be used to **classify a new sample** based on the weather, time of the week and time of the day.\n",
    "\n",
    "To transform a _prior_ probability into a _posterior_ probability, Naive Bayes' assumes all features are _uncorrelated_ so the _likelihoods_ can be simply multiplied with the _prior_.\n",
    "\n",
    "Multiplying the _prior_ with all the relevant _likelihoods_ gives you a _score_ for a class. To transform this score into the _posterior_ probability of that class, it needs to be normalised by the sum of _scores_ for each class.\n",
    "\n",
    "For example, to transform the prior probability of there being a traffic jam, into a posterior probability, given the evidence $(x_1, x_2, x_3)$ = (`Rainy`, `Weekend`, `Evening`), we would calculate:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(class = \\texttt{Yes}|\\texttt{Rainy}, \\texttt{Weekend}, \\texttt{Evening} ) &= \\frac{P(class=\\texttt{Yes})P(\\texttt{Rainy}|class=\\texttt{Yes})P(\\texttt{Weekend}|class=\\texttt{Yes})P(\\texttt{Evening}|class=\\texttt{Yes})}{\\sum_i{P(class=i)P(\\texttt{Rainy}|class=i)P(\\texttt{Weekend}|class=i)P(\\texttt{Evening}|class=i)}} \\\\\n",
    "&= \\frac{score(class=\\texttt{Yes})}{\\sum_i{score(class=i)}},\n",
    "\\end{align}\n",
    "$$\n",
    "with\n",
    "$$\n",
    "score(class=i|\\texttt{Rainy}, \\texttt{Weekend}, \\texttt{Evening})=P(class=i)P(\\texttt{Rainy}|class=i)P(\\texttt{Weekend}|class=i)P(\\texttt{Evening}|class=i).\n",
    "$$\n",
    "\n",
    "\n",
    "In general, if our sample is of form $x_1, x_2, x_3, ... x_n$, we can express the score as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "score(class=i|x_1, x_2, ... x_n) &=P(class=i)P(x_1|class=i)...P(x_n|class=i)  \\\\\n",
    "& = P(class=i) \\prod_{i=j..n}{P(x_j|class=i)},\n",
    "\\end{align}\n",
    "$$\n",
    "the product between the prior and the likelihoods of all the feature realisations from the sample.\n",
    "\n",
    "To retrieve the likelihood $P(\\texttt{Rainy}|class=i)$ (or for any other value of any other feature), you will need [`pandas.DataFrame.xs()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.xs.html) which allows you to select samples from the `DataFrame` by the _second_ key in the index (in our case, the value of the feature in the sample). For example, to retrieve the likelihood of `timeOfWeek` to be `Weekend`, you'll need to call `likelihood('timeOfWeek').xi('Weekend', level=1)`\n",
    "\n",
    "To sum up different scores, you can use [`pandas.Dataframe.sum()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sum.html).\n",
    "\n",
    "To return the column name of the column (class) with the highest score in a [`pandas.DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), you can use [`pandas.DataFrame.idxmax()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.idxmax.html)\n",
    "\n",
    "**Once finished, try classifying different samples than the one included in this example. Do the model predictions match your expectations?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(sample):\n",
    "    ##########################################################\n",
    "    ################## YOUR SOLUTION GOES HERE ###############\n",
    "    ##########################################################\n",
    "\n",
    "    # start by initialising your posterior scores to be equal to priors\n",
    "\n",
    "    # transform the priors to the posterior scores by looping through\n",
    "    # all the feature/column names (weather; timeOfWeek; timeOfDay -- stored in feature_cols)\n",
    "    # jointly with the feature values (the below example: 'Rainy' 'Weekend' 'Evening' -- given as sample)\n",
    "        # for each feature+value, multiply the posterior score with the\n",
    "        # likelihood of that feature having that specific value to update it\n",
    "\n",
    "    # (optional)\n",
    "    # to get the posterior probabilities from the posterior scores,\n",
    "    # normalise posterior scores per class by normalising (dividing\n",
    "    # the posterior scores per class with the sum of all posterior\n",
    "    # scores)\n",
    "    \n",
    "    # finally, return the class with the highest posterior/posterior score\n",
    "    # \n",
    "    return 0\n",
    "\n",
    "print(classify(('Rainy', 'Weekend', 'Evening')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero probability problem\n",
    "\n",
    "This problem can happen when we have a _very rare_ occurance of some feature value, which is _unrepresented_ (or, not sufficiently represented) in the dataset.\n",
    "\n",
    "Let's examine the dataset together and try and see what would be the logical classification of (`Snowy`, `Workday`, `Morning`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jams_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may say that it would make sense to predict `Yes`, there will be a traffic jam, because on every other `Workday` `Morning` in the dataset, there was a traffic jam.\n",
    "\n",
    "However, `Snowy` weather is underrepresented in the dataset -- it never co-occurs with `trafficJam`=`Yes`. If we look at the likelihoods for `weather`, we can notice that the $P(\\texttt{Snowy}|class=\\texttt{Yes})$ is missing (we can take that as it having a value zero). Because of this, our classifier will **never predict a traffic jam in snowy weather!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(likelihood('weather'))\n",
    "print(classify(('Snowy', 'Weekend', 'Morning')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2: Zero probability correction (extension)\n",
    "\n",
    "**Note: you do not have to demonstrate the solutions to this exercise to pass this workshop**\n",
    "\n",
    "To cope with the above situation, we can add a very small value, e.g. $1$, to all the counts when calculating _likelihoods_. If we have $c$ classes, this transforms the formula for likelihood into:\n",
    "$$\n",
    "P(\\texttt{Rainy} | class = i) = \\frac{N(\\texttt{Rainy},class=i)+1}{N(class=i)+c}.\n",
    "$$\n",
    "\n",
    "Make this change to the `likelihood` function implemented in **[Exercise 1c](#Exercise-1c:-Likelihoods)**, and re-run all the code cells up to this point. **How do the probabilities for a traffic jam in snowy weather look now?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes' classifier\n",
    "\n",
    "For this part of the workshop, we will work with a reduced dataset of [Algerian Forest Fires](https://archive.ics.uci.edu/ml/datasets/Algerian+Forest+Fires+Dataset++) [[1]](#References) (please download the file `fires_bejaia_ML.csv` from BlackBoard for this part). We will use the [`sklearn` implementation](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) of Gaussian Naive Bayes' classifier. The dataset describes the weather conditions in Bejaia region of Algeria, and the presence or absence of Forest Fires, and contains the following features:\n",
    "\n",
    "| Feature Name      | Value | Description |\n",
    "| :---------------- | :----- | ----------- |\n",
    "| `day`       | `int` | Day of the year. |\n",
    "| `month` | `int` | Month of the year. |\n",
    "| `year` | `int` | Calendar year. |\n",
    "| `Temperature` | `int`: $^\\circ C$ | Temperature on the day. |\n",
    "| `RH` | `int`: % | Relative humidity. |\n",
    "| `Ws` | `int`: km/h| Wind speed. |\n",
    "| `Rain` | `float`: mm | Amount of rain on the day. |\n",
    "| `FFMC` | `float` | Fine Fuel Moisture Code index. |\n",
    "| `DMC` | `float` | Duff Moisture Code index. |\n",
    "| `DC` | `float` |  Drought Code index. |\n",
    "| `ISI` | `float` | Initial Spread Index. |\n",
    "| `BUI` | `float` | Buildup Index. |\n",
    "| `FWI` | `float` | Fire Weather Index. |\n",
    "| `Classes` | `string` | Two classes: `fire` or `not fire`. |\n",
    "\n",
    "First, let us load the dataset and inspect some samples to see what we are dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires_df = pd.read_csv('fires_bejaia_ML.csv')\n",
    "fires_df.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also plot the relationship between some features.\n",
    "\n",
    "For example, let us look at the relationship between relative humidity (`RH`) and the `DMC` index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fires = fires_df[fires_df.Classes == 'fire']\n",
    "nofires = fires_df[fires_df.Classes == 'not fire']\n",
    "plt.xlabel(\"RH\")\n",
    "plt.ylabel(\"DMC\")\n",
    "plt.scatter(fires.RH, fires.DMC, color = \"red\", label = \"fires\", alpha = 0.3)\n",
    "plt.scatter(nofires.RH, nofires.DMC, color = \"lime\", label = \"no_fires\", alpha = 0.3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the data, we can probaby safely _ignore the date_. We can also transform the class labels into one-hot encoded labels with [`pandas.get_dummies()`](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html).\n",
    "To prepare our data for classification with the [Gaussian Naive Bayes' classifier](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html), let us then grab all the features _except_ the date, and class labels as zeros and ones.\n",
    "\n",
    "(Since we have only two classes, we can interpret our one-hot label for `fire` as a unifying grount truth for the dataset, where `1` means there was a fire and `0` means there was no fire. The one-hot label for `not fire` is exactly the label for `fire` but negated.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = fires_df[fires_df.columns[3:-1]]\n",
    "classes_df = pd.get_dummies(fires_df.Classes)\n",
    "\n",
    "X = data_df.values\n",
    "y = classes_df.fire.values\n",
    "\n",
    "#classes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also split the dataset into some training and testing samples. We will store our random selection in `train_idx` and `test_idx` so we can repeat exactly the same selection later. We will also use a fairly large size of the test set (60%) as we will be splitting it out further in the coming exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state = 3\n",
    "\n",
    "train_idx, test_idx = train_test_split(np.arange(0, X.shape[0]), test_size = 0.6, random_state = random_state)\n",
    "\n",
    "X_train, X_test = X[train_idx], X[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3a: Training GaussianNB\n",
    "\n",
    "Initialise a classification model with the `sklearn` implementation of [`GaussianNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html).\n",
    "\n",
    "Then, fit the model to the training data `X_train, y_train` using [`GaussianNB.fit()`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB.fit) method.\n",
    "\n",
    "Finally, predict the outputs for the testing data using [`GaussianNB.predict()`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB.predict) method, and calculate the confusion matrix using [`metrics.confusion_matrix()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html). You can also try outputing all the probabilities with [`GaussianNB.predict_proba()`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB.predict_proba)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "\n",
    "model_cont = # ...\n",
    "\n",
    "# fit the model to the data:\n",
    "# ...\n",
    "# predict the outputs:\n",
    "# ...\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_predict, y_test)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3b: Updating beliefs\n",
    "\n",
    "With Naive Bayes' classifier, it is easy to _update beliefs_, if new data becomes available. The `sklearn` implementation supports that through [`GaussianNB.partial_fit()`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB.partial_fit). This can also be useful when you are handling so much data, you can not fit it in your memory all at once. In this case, you can still fit a Gaussian Navie Bayes' model to the data, part by part.\n",
    "\n",
    "Perform the following steps:\n",
    "- Split the `X_test` and `y_test` _further_ using [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) **in half** . Use the same `random_state` as above for repeatability. Store the results into `X_train_new`, `X_test_new`, `y_train_new` and `y_test_new`\n",
    "- Initialise your model with a new instance of [`GaussianNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) and fit the model to (`X_train`, `y_train`) using the [`GaussianNB.partial_fit()`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB.partial_fit). This model will also need a **list of all classes**. In this case, you can simply pass `[0, 1]` for the third argument, or more generally, you can use [`numpy.unique()`](https://numpy.org/doc/stable/reference/generated/numpy.unique.html) on your training labels (`y_train`) if you are certain all the labels are contained in the training set.\n",
    "- Calculate the confusion matrix `cm` for this case, to convince yourself it is the same one as in [Exercise 4a](Exercise-4a:-Training-GaussianNB) when we used [`GaussianNB.fit()`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB.fit)\n",
    "- Also evaluate this model on the reduced testing set `X_test_new` and produce a confusion matrix `cm_small` by comparing these outputs to `y_test_new`.\n",
    "- Using [`GaussianNB.partial_fit()`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB.partial_fit) fit the model to further data (`X_train_new`, `y_train_new`).\n",
    "- Finally, evaluate the model on the the reduced testing set `X_test_new` again and produce an updated confusion matrix `cm_after` by comparing these outputs to `y_test_new`. Compare the performance of the model before and after training on additional data\n",
    "\n",
    "_Note: another (cheaty) reason you are asked to use the same `random_state=3` is because this random state actually results in an improvement to the classifier. As we are using quite a small dataset, not all random seeds do -- but adding more data generally helps the classifier in the wild._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(\n",
    "        X_test, y_test, test_size = 0.5, random_state = random_state)\n",
    "\n",
    "model_cont = # ... initialise the model\n",
    "# fit the model to the (old) training data X_train, y_train using partial_fit:\n",
    "# ...\n",
    "\n",
    "# calculate and store the confusion matrix on X_test/y_test\n",
    "cm = # ..\n",
    "\n",
    "# calculate and store the confusion matrix on X_test_new/y_test_new\n",
    "cm_small = # ...\n",
    "\n",
    "# do not change the following lines -- plotting the confusion matrix\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "\n",
    "# do not change the following lines -- plotting the (other) confusion matrix\n",
    "disp_small = metrics.ConfusionMatrixDisplay(confusion_matrix=cm_small)\n",
    "disp_small.plot()\n",
    "\n",
    "# fit the model to more training data (X_train_new, y_train_new) using partial_fit:\n",
    "# ...\n",
    "\n",
    "# calculate and store the confusion matrix on X_test_new/y_test_new\n",
    "cm_after = # ...\n",
    "\n",
    "# do not change the following lines -- plotting the (other) confusion matrix\n",
    "disp_after = metrics.ConfusionMatrixDisplay(confusion_matrix=cm_after)\n",
    "disp_after.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3c: Removing correlated features\n",
    "\n",
    "The Naive Bayes' classifier assumes all features are _uncorrelated_. However, this is often not true in the real world. While it still often works well enough in practice (as seen on the example above), having highly correlated features in the dataset does not contribute much to its performance.\n",
    "\n",
    "In this exercise, you should first examine the correlation matrix between all the different features obtained with [`pandas.DataFrame.corr()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html) (just run the next cell for now and study it! and read on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires_df.head()\n",
    "fires_df[fires_df.columns[3:-1]].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation can be:\n",
    "- 1: this means that you can calculate the value of one feature _exactly_ from the value of the other feature\n",
    "- close to 1: this means that the features are _highly correlated_ -- you can _predict_ one from the other with a _high accuracy_\n",
    "- close to -1: this means that the features are _highly negatively correlated_ -- you can still _predict_ one from the other with a _high accuracy_, just that as the first feature increases, the second decreases\n",
    "- close to 0: there is little to no relationship between the two features.\n",
    "\n",
    "Examining the above correlation table, select only $5$ features for the reduced training set `X_reduced`. (It will still contain information about all the days from the dataset, but will be reduced in the number of features used). You can simply do this by passing a list of features into the dataframe, e.g: `fires_df[['Ws, 'ISI']]` (and retrieving the data through the `values` attribute).\n",
    "\n",
    "Initialise a new instance of [`GaussianNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) and fit the model to (`X_train_train`, `y_train`) using the [`GaussianNB.fit()`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB.fit) method. Then, predict the outputs for `X_reduced_test` with [`GaussianNB.predict()`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB.predict) method, and calculate the confusion matrix using [`metrics.confusion_matrix()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html), by comparing those outputs with `y_test`.\n",
    "\n",
    "Compare the confusion matrix obtained with a reduced set of features with the one from [Exercise 3a](#Exercise-3a:-Training-GaussianNB). **Did the classifier performance change much?**\n",
    "\n",
    "Try experimenting with different feature combinations in the classifier, and see how little features you can use while still obtaining good classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = fires_df.values # replace this line to select only 5 features from the dataset\n",
    "# for example, to select features 'RH' and 'Ws', you could call:\n",
    "# X_reduced = fires_df[['RH', 'Ws']].values\n",
    "\n",
    "# select the reduced portions of the training/testing features\n",
    "X_reduced_train, X_reduced_test = X_reduced[train_idx], X_reduced[test_idx]\n",
    "\n",
    "# initialise a gaussian Naive Bayes classifier\n",
    "model_reduced = # ...\n",
    "\n",
    "# fit the model to the data (X_reduced_train, y_train):\n",
    "# ...\n",
    "\n",
    "# predict the results and calculate confusion matrix:\n",
    "cm_reduced = # ...\n",
    "\n",
    "disp_after = metrics.ConfusionMatrixDisplay(confusion_matrix=cm_reduced)\n",
    "disp_after.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a class (extension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have implemented all the steps in the Naive Bayes' classifier for categorical features in the [Categorical Naive Bayes' classifier](#Categorical-Naive-Bayes'-classifier) section. For an additional challenge, try and puting them together in a class.\n",
    "\n",
    "**Note: you do not have to demonstrate the solutions to this section to pass this workshop**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4: Implementing a class (extension)\n",
    "\n",
    "Implement a `myCategoricalNB` class, which operates on a [`pandas.DataFrame`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html). It should have the standard `__init__()`, `fit()` and `predict()` methods.\n",
    "\n",
    "Also make a `predict_proba()` method which accepts the same arguments as `predict()` but returns a [`pandas.DataFrame`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) containing probabilities for both of the classes.\n",
    "\n",
    "I suggest you also use a helper method `__likelihood()` with a similar function to the `likelihood` function defined in [Exercise 1c](Exercise-1c:-Likelihoods).\n",
    "\n",
    "You can use the cell marked with **TESTING CELL** below to test how your classifier works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCategoricalNB:\n",
    "    def __init__(self):\n",
    "        return\n",
    "  \n",
    "    def __likelihood(self, feature):\n",
    "        return 0 # replace with your implementation\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \n",
    "        # add your implementation here\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        X = np.array(X)\n",
    "        \n",
    "        # This just ensures this works for single samples and\n",
    "        # for list\n",
    "        if len(X.shape) >= 2:\n",
    "            return np.array([self.predict(x) for x in X])\n",
    "        \n",
    "        # add your implementation for predicting for a single sample here:\n",
    "        \n",
    "        return 0 # and remove this line...\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        X = np.array(X)\n",
    "        \n",
    "        # This just ensures this works for single samples and\n",
    "        # for list\n",
    "        if len(X.shape) >= 2:\n",
    "            return pd.concat([self.predict_proba(x) for x in X], axis=1)\n",
    "        \n",
    "        # add your implementation for predicting for a single sample here:\n",
    "        \n",
    "        return 0 # and remove this line..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TESTING CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = myCategoricalNB()\n",
    "model.fit(jams_df)\n",
    "model.predict(('Rainy', 'Weekend', 'Evening'))\n",
    "model.predict_proba(('Snowy', 'Workday', 'Morning'))\n",
    "\n",
    "samples = [('Rainy', 'Weekend', 'Evening'), \n",
    "           ('Snowy', 'Workday', 'Morning'),\n",
    "           ('Clear', 'Weekend', 'Lunch'),\n",
    "           ('Clear', 'Workday', 'Morning'),\n",
    "           ('Rainy', 'Workday', 'Lunch'),\n",
    "           ('Snowy', 'Workday', 'Lunch'),]\n",
    "\n",
    "print(model.predict(samples))\n",
    "model.predict_proba(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] _Abid, Faroudja, and Nouma Izeboudjen. \"Predicting forest fire in algeria using data mining techniques: Case study of the decision tree algorithm.\" International Conference on Advanced Intelligent Systems for Sustainable Development. Springer, Cham, 2020._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
